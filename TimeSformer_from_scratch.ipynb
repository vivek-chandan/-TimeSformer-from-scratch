{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMyIDt8E2b2s40POaoha/17",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vivek-chandan/-TimeSformer-from-scratch/blob/main/TimeSformer_from_scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from einops import rearrange\n",
        "from tqdm import tqdm\n",
        "import timm\n"
      ],
      "metadata": {
        "id": "BDzF-23xGcv9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available else \"cpu\""
      ],
      "metadata": {
        "id": "uo81PsvKHRwL"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# video data part currently not coding I will do it after transorfmer module"
      ],
      "metadata": {
        "id": "dWQT8YA_Jdh9"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-ttAoAgCJcjL"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer Modules"
      ],
      "metadata": {
        "id": "GmRrX_3wJvgw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TimeSformerBlock(nn.Module):\n",
        "    def __init__(self, dim, heads):\n",
        "      super().__init__()\n",
        "\n",
        "      self.temporal_attn = nn.MultiheadAttention(dim, heads, batch_first = True)\n",
        "      self.spetial_attn = nn.MultiheadAttention(dim, heads, batch_first = True)\n",
        "\n",
        "      self.norm1 = nn.LayerNorm(dim)\n",
        "      self.norm2 = nn.LayerNorm(dim)\n",
        "      self.norm3 = nn.LayerNorm(dim)\n",
        "\n",
        "      self.mlp = nn.Sequential(\n",
        "          nn.Linear(dim, dim * 4),\n",
        "          nn.GELU(),\n",
        "          nn.Linear(dim * 4, dim)\n",
        "      )\n",
        "\n",
        "    def forward(self, x):\n",
        "      # temporal attention\n",
        "      B, T, N, D = x.shape # BatchDim, TemporalDim , NumOfPatches, EmbeddingDim\n",
        "      xt = rearrange(x, \"b t n d -> (b  n) t d\")\n",
        "      xt = self.temporal_attn(xt, xt, xt)[0]\n",
        "      xt = rearrange(xt, \"(b n) t d -> b t n d\", b = B, n = N)\n",
        "      x = x + self.norm1(xt)\n",
        "\n",
        "      # spatial attention\n",
        "\n",
        "      xs = rearrange(x, \"b t n d -> (b t) n d\")\n",
        "      xs = self.spetial_attn(xs, xs, xs)[0]\n",
        "      xs = rearrange(xs, \"(b t) n d -> b t n d\", b = B, t = T)\n",
        "      x = x + self.norm2(xs)\n",
        "\n",
        "      # mlp\n",
        "\n",
        "      xl = self.mlp(x)\n",
        "      x = x + self.norm3(xl)\n",
        "\n",
        "      return x"
      ],
      "metadata": {
        "id": "kIlmFgAaJtFi"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn # Added import for nn\n",
        "\n",
        "# TimeSformer\n",
        "\n",
        "class TimeSformer(nn.Module): # Corrected Modules to Module\n",
        "  def __init__(self ,\n",
        "               num_classes,\n",
        "               num_frames=8,\n",
        "               img_size = 224,\n",
        "               patch_size = 16 ,\n",
        "               embed_dim = 768,\n",
        "               depths =12,\n",
        "               heads = 12,):\n",
        "\n",
        "    super(). __init__()\n",
        "\n",
        "    self.num_frames = num_frames\n",
        "    self.patch_size = patch_size\n",
        "    self.embed_dim = embed_dim\n",
        "    self.img_size = img_size\n",
        "    self.num_patches = (img_size // patch_size) ** 2\n",
        "\n",
        "    self.patch_embed = nn.Conv2d(3, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim) )\n",
        "\n",
        "    self.time_embed = nn.Parameter(torch.randn(1, self.num_frames + 1, embed_dim ))\n",
        "    self.space_embed = nn.Parameter(torch.randn(1, self.num_patches , embed_dim ))\n",
        "\n",
        "    self.blocks = nn.ModuleList([\n",
        "        TimeSformerBlock(embed_dim, heads) for _ in range(depths) # Corrected TimeSformerBlock to TransformerBlock\n",
        "        ]\n",
        "        )\n",
        "    self.norm = nn.LayerNorm(embed_dim)\n",
        "    self.head = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "   B, T, C, H, W = x.shape # B-> batch , T -> temporal , C -> chanel, H-> height ,W->width\n",
        "   x = x.view(B * T, C, H, W)\n",
        "   x = self.patch_embed(x)\n",
        "   x= x.flatten(2).transpose(1, 2)\n",
        "   x = x.view(B, T, -1, self.embed_dim)\n",
        "\n",
        "   # add position embedding excludding cls token\n",
        "   x = x + self.time_embed[:,1:T+1, None, :]+self.space_embed[:, None, :, :]\n",
        "\n",
        "   # add cls token\n",
        "   cls = self.cls_token.expand(B, -1, self.num_patches, -1)\n",
        "   cls = cls + self.time_embed[:, :1, None, :]\n",
        "   # preprend cls token\n",
        "   x = torch.cat((cls, x), dim=1)\n",
        "\n",
        "   #transformer blocks\n",
        "   for block in self.blocks:\n",
        "     x = block(x)\n",
        "\n",
        "   cls_out = self.norm(x[:, 0,0])\n",
        "   out = self.head(cls_out)\n",
        "\n",
        "   return out"
      ],
      "metadata": {
        "id": "_sPV7ToHOAp6"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_Z7lV3c3s_d1"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "z4hA0AfNv2x5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = TimeSformer(num_classes=101,\n",
        "                    num_frames= 8,\n",
        "                    embed_dim= 768,\n",
        "                    depth = 12,\n",
        "                    heads =12).to(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        },
        "id": "72cCsd1HONt9",
        "outputId": "97b51967-3999-4048-f5a2-4870292d5f6f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'train_dataset' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2792043597.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m model = TimeSformer(num_classes=len(train_dataset.classes),\n\u001b[0m\u001b[1;32m      2\u001b[0m                     \u001b[0mnum_frames\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                     \u001b[0membed_dim\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;36m768\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                     \u001b[0mdepth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                     heads =12)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_dataset' is not defined"
          ]
        }
      ]
    }
  ]
}